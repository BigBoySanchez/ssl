# 9/25 Prep

## GOAL
Run VerifyMatch on some crisis datasets...
- crisismmd
- humAID

## GPT Explanation
Awesome â€” letâ€™s redo the â€œwork backwardsâ€ plan and explicitly weave **calibration** into the pipeline youâ€™ve got (notes + `train.py`).

# ğŸ¯ Goal (re-stated)

Run **VerifyMatch** on **CrisisMMD** and **humAID** and produce **calibrated** results (accuracy/F1 + ECE/Brier-style confidence quality) you can trust. Your notes already set the target datasetsã€ã€‘.

---

# ğŸ”„ Work backwards from â€œdoneâ€

## 0) Final deliverables (what â€œdoneâ€ looks like)

* Calibrated test outputs for each dataset: JSONL with `true`, `pred`, `logits`, `probs`, `conf`, written by `train.py` evaluate stepã€ã€‘ã€ã€‘.
* A saved **calibration parameter** file (e.g., temperature Ï„).
* Reported metrics (Accuracy, Macro-F1 from `train.py`; plus your calibration metrics computed from calibrated probs)ã€ã€‘.
* (If using SSL/VerifyMatch) Training-time logs that used **calibrated confidence** to decide high/low buckets for unlabeled data (i.e., the step that softmaxes logits and takes the max confidence)ã€ã€‘.

---

## 1) Calibrated evaluation & reporting

1. **Evaluate â†’ dump raw (uncalibrated) predictions**
   Use `--do_evaluate` to write JSONL with logits/probs/conf; `train.py` does this already for test/dev and prints Accuracy/F1ã€ã€‘ã€ã€‘.

2. **Fit calibration on dev predictions**

   * Train **post-hoc** calibration (e.g., temperature scaling) using the dev JSONL (inputs: `logits`/`true`).
   * Save Ï„ (temperature).

3. **Apply calibration to test predictions**

   * Re-evaluate (or reuse raw test JSONL), apply Ï„ to logits â†’ `softmax(logits/Ï„)` to get calibrated `probs`, recompute ECE/Brier/coverage-style metrics.
   * Keep both raw and calibrated JSONLs for auditing.

---

## 2) Calibrated VerifyMatch (SSL) training

VerifyMatch decides **high/low-confidence** unlabeled samples using softmax probabilities derived from logits, with optional sharpening `T`ã€ã€‘ and then takes the max as `verifier_prob`ã€ã€‘.
To align that decision with reality, apply your learned **Ï„** inside this loop:

* Replace:
  `tmp_labels2 = F.softmax(logits2, dim=-1)`
* With (conceptually):
  `tmp_labels2 = F.softmax(logits2 / Ï„, dim=-1)`
* (Then your existing sharpening with `args.T` still applies)ã€ã€‘.

This ensures the **confidence** used for filtering/mixing unlabeled examples is calibrated before you threshold/split (you can still use your CLI threshold `--th` and sharpening `--T`)ã€ã€‘.

Artifacts youâ€™ll already get from the SSL path (progress files, confidence tracking) continue to work and become more meaningful when confidence is calibrated.

---

## 3) Supervised training + checkpointing (the base model you calibrate)

* Train on your labeled split(s); `train.py` saves the best checkpoint based on dev accuracyã€ã€‘.
* Later, `--do_evaluate` loads that checkpoint and emits the JSONL outputsã€ã€‘.

Key flags youâ€™ll use a lot:

* `--ssl` to enable the VerifyMatch path during training (semi-supervised)ã€ã€‘
* `--th`, `--T` to control thresholding and sharpeningã€ã€‘

---

## 4) Data prep (match your datasets to the processors)

Your code expects **pair** inputs (sentence1, sentence2, label\[, guid]) for NLI/QQP-style tasks; the tokenizer builds `[CLS] s1 [SEP] s2 [SEP]` pairs via `encode_pair_inputs`ã€ã€‘ã€ã€‘.
Pick a processor whose label schema fits your data:

* **RTEProcessor** expects labels in `{'entailment','not_entailment'}` which it maps to {1,0}ã€ã€‘ã€ã€‘.
* **QQPProcessor** expects binary labels `'0'/'1'`ã€ã€‘ã€ã€‘.

Format **CrisisMMD**/**humAID** into TSVs compatible with the chosen processor; then point `--train_path/--dev_path/--test_path` to those files. The code will load and batch them through `TextDataset` â†’ processor â†’ `encode_pair_inputs`ã€ã€‘ã€ã€‘.

---

# ğŸ› ï¸ Concrete commands (templates)

## A) Supervised baseline (per dataset)

Train â†’ best ckpt â†’ dev dump for calibration:

```bash
python train.py \
  --device 0 \
  --model roberta-base \
  --task RTE \
  --train_path data/<ds>/train.tsv \
  --dev_path data/<ds>/dev.tsv \
  --test_path data/<ds>/test.tsv \
  --ckpt_path ckpts/<ds>_rte_roberta.pt \
  --output_path outputs/<ds>_dev_raw.jsonl \
  --do_train --do_evaluate
```

This writes JSONL with `true/pred/conf/logits/probs` for dev/test as used by your evaluation pathã€ã€‘.

Fit temperature Ï„ on dev JSONL, then apply to the test JSONL (using your `calibrate.py`).

## B) SSL with VerifyMatch (calibrated)

```bash
python train.py \
  --device 0 \
  --model roberta-base \
  --task RTE \
  --labeled_train_path data/<ds>/train.tsv \
  --unlabeled_train_path data/<ds>/unlabeled.tsv \
  --dev_path data/<ds>/dev.tsv \
  --test_path data/<ds>/test.tsv \
  --ckpt_path ckpts/<ds>_ssl_ckpt.pt \
  --output_path outputs/<ds>_ssl_test_raw.jsonl \
  --ssl --th 0.7 --T 0.1 \
  --do_train --do_evaluate
```

Then apply Ï„ inside the VerifyMatch loop at the point where `tmp_labels2 = F.softmax(logits2, dim=-1)` is computed (insert `/ Ï„` before softmax) and keep `--th`/`--T` as knobsã€ã€‘.

---

# âœ… Minimal checklist to execute

* [ ] **Prep data** into the pair-TSV format your chosen processor expects (RTE/QQP)ã€ã€‘ã€ã€‘
* [ ] **Train baseline** (supervised) â†’ best ckpt saved on dev accuracyã€ã€‘
* [ ] **Evaluate** to write **dev** & **test** JSONLs with raw `logits/probs/conf`ã€ã€‘
* [ ] **Fit Ï„ on dev**, **apply** to test and to the **VerifyMatch** loop (divide logits by Ï„ before softmax)ã€ã€‘
* [ ] **Report** Accuracy/F1 from `train.py` plus **calibration** metrics from calibrated probsã€ã€‘

If you want, paste a snippet of your `calibrate.py` flags and Iâ€™ll drop in the exact one-liners (fit/apply) and the tiny code change to `train.py` at the **pseudolabel generation** site so VerifyMatch uses calibrated confidence.
