{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd892375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from pathlib import Path\n",
    "\n",
    "my_df = pd.read_csv(r'..\\data\\humaid\\plabel\\train\\union.tsv', sep='\\t')\n",
    "their_folder = Path(\"..\\data\\humaid\\k_zero_shot\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "for event_file in their_folder.glob(\"*.tsv\"):\n",
    "    their_df = pd.read_csv(event_file, sep='\\t')\n",
    "    event = '_'.join(event_file.stem.split('_')[:3])\n",
    "\n",
    "    merged = pd.merge(my_df[my_df['event'] == event], their_df, on='tweet_id')\n",
    "    merged = merged[['class_label', 'label', 'gpt5_label']]\n",
    "\n",
    "    my_f1 = f1_score(merged['class_label'], merged['label'], average='macro')\n",
    "    their_f1 = f1_score(merged['class_label'], merged['gpt5_label'], average='macro')\n",
    "\n",
    "    rows.append({'event': event, 'mine': my_f1, 'theirs': their_f1})\n",
    "\n",
    "pd.DataFrame(rows).to_clipboard()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "934e80bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/10423 tweets...\n",
      "Processed 100/10423 tweets...\n",
      "Processed 150/10423 tweets...\n",
      "Processed 200/10423 tweets...\n",
      "Processed 250/10423 tweets...\n",
      "Processed 300/10423 tweets...\n",
      "Processed 350/10423 tweets...\n",
      "Processed 400/10423 tweets...\n",
      "Processed 450/10423 tweets...\n",
      "Processed 500/10423 tweets...\n",
      "Processed 550/10423 tweets...\n",
      "Processed 600/10423 tweets...\n",
      "Processed 650/10423 tweets...\n",
      "Processed 700/10423 tweets...\n",
      "Processed 750/10423 tweets...\n",
      "Processed 800/10423 tweets...\n",
      "Processed 850/10423 tweets...\n",
      "Processed 900/10423 tweets...\n",
      "Processed 950/10423 tweets...\n",
      "Processed 1000/10423 tweets...\n",
      "Processed 1050/10423 tweets...\n",
      "Processed 1100/10423 tweets...\n",
      "Processed 1150/10423 tweets...\n",
      "Processed 1200/10423 tweets...\n",
      "Processed 1250/10423 tweets...\n",
      "Processed 1300/10423 tweets...\n",
      "Processed 1350/10423 tweets...\n",
      "Processed 1400/10423 tweets...\n",
      "Processed 1450/10423 tweets...\n",
      "Processed 1500/10423 tweets...\n",
      "Processed 1550/10423 tweets...\n",
      "Processed 1600/10423 tweets...\n",
      "Processed 1650/10423 tweets...\n",
      "Processed 1700/10423 tweets...\n",
      "Processed 1750/10423 tweets...\n",
      "Processed 1800/10423 tweets...\n",
      "Processed 1850/10423 tweets...\n",
      "Processed 1900/10423 tweets...\n",
      "Processed 1950/10423 tweets...\n",
      "Processed 2000/10423 tweets...\n",
      "Processed 2050/10423 tweets...\n",
      "Processed 2100/10423 tweets...\n",
      "Processed 2150/10423 tweets...\n",
      "Processed 2200/10423 tweets...\n",
      "Processed 2250/10423 tweets...\n",
      "Processed 2300/10423 tweets...\n",
      "Processed 2350/10423 tweets...\n",
      "Processed 2400/10423 tweets...\n",
      "Processed 2450/10423 tweets...\n",
      "Processed 2500/10423 tweets...\n",
      "Processed 2550/10423 tweets...\n",
      "Processed 2600/10423 tweets...\n",
      "Processed 2650/10423 tweets...\n",
      "Processed 2700/10423 tweets...\n",
      "Processed 2750/10423 tweets...\n",
      "Processed 2800/10423 tweets...\n",
      "Processed 2850/10423 tweets...\n",
      "Processed 2900/10423 tweets...\n",
      "Processed 2950/10423 tweets...\n",
      "Processed 3000/10423 tweets...\n",
      "Processed 3050/10423 tweets...\n",
      "Processed 3100/10423 tweets...\n",
      "Processed 3150/10423 tweets...\n",
      "Processed 3200/10423 tweets...\n",
      "Processed 3250/10423 tweets...\n",
      "Processed 3300/10423 tweets...\n",
      "Processed 3350/10423 tweets...\n",
      "Processed 3400/10423 tweets...\n",
      "Processed 3450/10423 tweets...\n",
      "Processed 3500/10423 tweets...\n",
      "Processed 3550/10423 tweets...\n",
      "Processed 3600/10423 tweets...\n",
      "Processed 3650/10423 tweets...\n",
      "Processed 3700/10423 tweets...\n",
      "Processed 3750/10423 tweets...\n",
      "Processed 3800/10423 tweets...\n",
      "Processed 3850/10423 tweets...\n",
      "Processed 3900/10423 tweets...\n",
      "Processed 3950/10423 tweets...\n",
      "Processed 4000/10423 tweets...\n",
      "Processed 4050/10423 tweets...\n",
      "Processed 4100/10423 tweets...\n",
      "Processed 4150/10423 tweets...\n",
      "Processed 4200/10423 tweets...\n",
      "Processed 4250/10423 tweets...\n",
      "Processed 4300/10423 tweets...\n",
      "Processed 4350/10423 tweets...\n",
      "Processed 4400/10423 tweets...\n",
      "Processed 4450/10423 tweets...\n",
      "Processed 4500/10423 tweets...\n",
      "Processed 4550/10423 tweets...\n",
      "Processed 4600/10423 tweets...\n",
      "Processed 4650/10423 tweets...\n",
      "Processed 4700/10423 tweets...\n",
      "Processed 4750/10423 tweets...\n",
      "Processed 4800/10423 tweets...\n",
      "Processed 4850/10423 tweets...\n",
      "Processed 4900/10423 tweets...\n",
      "Processed 4950/10423 tweets...\n",
      "Processed 5000/10423 tweets...\n",
      "Processed 5050/10423 tweets...\n",
      "Processed 5100/10423 tweets...\n",
      "Processed 5150/10423 tweets...\n",
      "Processed 5200/10423 tweets...\n",
      "Processed 5250/10423 tweets...\n",
      "Processed 5300/10423 tweets...\n",
      "Processed 5350/10423 tweets...\n",
      "Processed 5400/10423 tweets...\n",
      "Processed 5450/10423 tweets...\n",
      "Processed 5500/10423 tweets...\n",
      "Processed 5550/10423 tweets...\n",
      "Processed 5600/10423 tweets...\n",
      "Processed 5650/10423 tweets...\n",
      "Processed 5700/10423 tweets...\n",
      "Processed 5750/10423 tweets...\n",
      "Processed 5800/10423 tweets...\n",
      "Processed 5850/10423 tweets...\n",
      "Processed 5900/10423 tweets...\n",
      "Processed 5950/10423 tweets...\n",
      "Processed 6000/10423 tweets...\n",
      "Processed 6050/10423 tweets...\n",
      "Processed 6100/10423 tweets...\n",
      "Processed 6150/10423 tweets...\n",
      "Processed 6200/10423 tweets...\n",
      "Processed 6250/10423 tweets...\n",
      "Processed 6300/10423 tweets...\n",
      "Processed 6350/10423 tweets...\n",
      "Processed 6400/10423 tweets...\n",
      "Processed 6450/10423 tweets...\n",
      "Processed 6500/10423 tweets...\n",
      "Processed 6550/10423 tweets...\n",
      "Processed 6600/10423 tweets...\n",
      "Processed 6650/10423 tweets...\n",
      "Processed 6700/10423 tweets...\n",
      "Processed 6750/10423 tweets...\n",
      "Processed 6800/10423 tweets...\n",
      "Processed 6850/10423 tweets...\n",
      "Processed 6900/10423 tweets...\n",
      "Processed 6950/10423 tweets...\n",
      "Processed 7000/10423 tweets...\n",
      "Processed 7050/10423 tweets...\n",
      "Processed 7100/10423 tweets...\n",
      "Processed 7150/10423 tweets...\n",
      "Processed 7200/10423 tweets...\n",
      "Processed 7250/10423 tweets...\n",
      "Processed 7300/10423 tweets...\n",
      "Processed 7350/10423 tweets...\n",
      "Processed 7400/10423 tweets...\n",
      "Processed 7450/10423 tweets...\n",
      "Processed 7500/10423 tweets...\n",
      "Processed 7550/10423 tweets...\n",
      "Processed 7600/10423 tweets...\n",
      "Processed 7650/10423 tweets...\n",
      "Processed 7700/10423 tweets...\n",
      "Processed 7750/10423 tweets...\n",
      "Processed 7800/10423 tweets...\n",
      "Processed 7850/10423 tweets...\n",
      "Processed 7900/10423 tweets...\n",
      "Processed 7950/10423 tweets...\n",
      "Processed 8000/10423 tweets...\n",
      "Processed 8050/10423 tweets...\n",
      "Processed 8100/10423 tweets...\n",
      "Processed 8150/10423 tweets...\n",
      "Processed 8200/10423 tweets...\n",
      "Processed 8250/10423 tweets...\n",
      "Processed 8300/10423 tweets...\n",
      "Processed 8350/10423 tweets...\n",
      "Processed 8400/10423 tweets...\n",
      "Processed 8450/10423 tweets...\n",
      "Processed 8500/10423 tweets...\n",
      "Processed 8550/10423 tweets...\n",
      "Processed 8600/10423 tweets...\n",
      "Processed 8650/10423 tweets...\n",
      "Processed 8700/10423 tweets...\n",
      "Processed 8750/10423 tweets...\n",
      "Processed 8800/10423 tweets...\n",
      "Processed 8850/10423 tweets...\n",
      "Processed 8900/10423 tweets...\n",
      "Processed 8950/10423 tweets...\n",
      "Processed 9000/10423 tweets...\n",
      "Processed 9050/10423 tweets...\n",
      "Processed 9100/10423 tweets...\n",
      "Processed 9150/10423 tweets...\n",
      "Processed 9200/10423 tweets...\n",
      "Processed 9250/10423 tweets...\n",
      "Processed 9300/10423 tweets...\n",
      "Processed 9350/10423 tweets...\n",
      "Processed 9400/10423 tweets...\n",
      "Processed 9450/10423 tweets...\n",
      "Processed 9500/10423 tweets...\n",
      "Processed 9550/10423 tweets...\n",
      "Processed 9600/10423 tweets...\n",
      "Processed 9650/10423 tweets...\n",
      "Processed 9700/10423 tweets...\n",
      "Processed 9750/10423 tweets...\n",
      "Processed 9800/10423 tweets...\n",
      "Processed 9850/10423 tweets...\n",
      "Processed 9900/10423 tweets...\n",
      "Processed 9950/10423 tweets...\n",
      "Processed 10000/10423 tweets...\n",
      "Processed 10050/10423 tweets...\n",
      "Processed 10100/10423 tweets...\n",
      "Processed 10150/10423 tweets...\n",
      "Processed 10200/10423 tweets...\n",
      "Processed 10250/10423 tweets...\n",
      "Processed 10300/10423 tweets...\n",
      "Processed 10350/10423 tweets...\n",
      "Processed 10400/10423 tweets...\n",
      "Done. Saved to ../gpt4o_mini_predictions.tsv.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import csv, pandas as pd\n",
    "import os, time, random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Initialize API client ---\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# --- Load HumAID test data ---\n",
    "gold_table = pd.read_csv(\n",
    "    \"../data/humaid/joined/test.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    quoting=csv.QUOTE_NONE\n",
    ")\n",
    "gold_table = gold_table[gold_table[\"class_label\"] != \"other_relevant_information\"].reset_index(drop=True)\n",
    "\n",
    "# --- Define classification prompt template ---\n",
    "base_prompt = \"\"\"Read the category names and their definitions below, then classify the following tweet into the appropriate category. \n",
    "In your response, mention only the category name.\n",
    "\n",
    "Category name: category definition\n",
    "- Caution and advice: Reports of warnings issued or lifted, guidance and tips related to the disaster.\n",
    "- Sympathy and support: Tweets with prayers, thoughts, and emotional support.\n",
    "- Requests or urgent needs: Reports of urgent needs or supplies such as food, water, clothing, money, etc.\n",
    "- Displaced people and evacuations: People who have relocated due to the crisis, even for a short time.\n",
    "- Injured or dead people: Reports of injured or dead people due to the disaster.\n",
    "- Missing or found people: Reports of missing or found people due to the disaster.\n",
    "- Infrastructure and utility damage: Reports of any type of damage to infrastructure such as buildings, houses, roads, power lines, etc.\n",
    "- Rescue volunteering or donation effort: Reports of any type of rescue, volunteering, or donation efforts.\n",
    "- Not humanitarian: If the tweet does not convey humanitarian aid-related information.\n",
    "\n",
    "Tweet: {tweet_text}\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "# --- Define classifier function ---\n",
    "# def classify_tweet(tweet, max_retries=3):\n",
    "#     return \"TEST\"\n",
    "\n",
    "def classify_tweet(tweet, max_retries=3):\n",
    "    prompt = base_prompt.replace(\"{tweet_text}\", tweet)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "                max_tokens=10,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip().replace(' ', '_').lower()\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt + random.random()\n",
    "            print(f\"Error: {e} â€” retrying in {wait:.1f}s\")\n",
    "            time.sleep(wait)\n",
    "    return \"ERROR\"\n",
    "\n",
    "# --- Run predictions ---\n",
    "labels = []\n",
    "for i, row in gold_table.iterrows():\n",
    "    pred = classify_tweet(str(row[\"tweet_text\"]))\n",
    "    if(pred == \"ERROR\"):\n",
    "        raise RuntimeError(f\"Error encountered at row {i}. Stopping run.\")\n",
    "    labels.append(pred)\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"Processed {i+1}/{len(gold_table)} tweets...\")\n",
    "        # Optional checkpoint save\n",
    "        pd.DataFrame({\"tweet_text\": gold_table[\"tweet_text\"][:i+1], \"prediction\": labels}).to_csv(\n",
    "            \"../gpt4o_mini_predictions_partial.tsv\", index=False, sep=\"\\t\"\n",
    "        )\n",
    "\n",
    "# --- Save final predictions ---\n",
    "gold_table[\"prediction\"] = labels\n",
    "gold_table.to_csv(\"../gpt4o_mini_predictions.tsv\", index=False, sep=\"\\t\")\n",
    "print(\"Done. Saved to ../gpt4o_mini_predictions.tsv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f44f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
