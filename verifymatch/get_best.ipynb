{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0f5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any, Tuple, List, Optional\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def ece_function(\n",
    "    jsonl_path: str,\n",
    "    n_bins: int = 4,\n",
    "    conf_key: str = \"conf\",\n",
    "    pred_key: str = \"pred\",\n",
    "    label_key: str = \"label\",\n",
    "    skip_invalid: bool = True,\n",
    "    return_details: bool = False,\n",
    ") -> float | Tuple[float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Compute Expected Calibration Error (ECE) from a JSONL file where each line is a JSON object like:\n",
    "        {\"label\": 1, \"pred\": 5, \"conf\": 0.143}\n",
    "\n",
    "    This computes the standard top-1 ECE:\n",
    "      - confidence = `conf` (assumed to be max softmax probability for the predicted class)\n",
    "      - correctness = (pred == label)\n",
    "      - uniform bins over [0, 1]\n",
    "\n",
    "    Args:\n",
    "        jsonl_path: Path to the .jsonl file.\n",
    "        n_bins: Number of uniform confidence bins (paper you referenced uses 4).\n",
    "        conf_key: JSON key for confidence value.\n",
    "        pred_key: JSON key for predicted class.\n",
    "        label_key: JSON key for true label.\n",
    "        skip_invalid: If True, skip malformed lines / missing keys / out-of-range conf.\n",
    "                      If False, raise a ValueError on the first invalid line.\n",
    "        return_details: If True, also return per-bin stats for debugging/plotting.\n",
    "\n",
    "    Returns:\n",
    "        If return_details is False:\n",
    "            ece (float)\n",
    "        If return_details is True:\n",
    "            (ece, details_dict)\n",
    "    \"\"\"\n",
    "    preds: List[int] = []\n",
    "    labels: List[int] = []\n",
    "    confs: List[float] = []\n",
    "\n",
    "    def _handle_invalid(msg: str, line_no: int, line: str) -> None:\n",
    "        if skip_invalid:\n",
    "            return\n",
    "        raise ValueError(f\"[Line {line_no}] {msg}. Line: {line.strip()[:200]}\")\n",
    "\n",
    "    # ---- Load JSONL ----\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                _handle_invalid(\"Invalid JSON\", line_no, line)\n",
    "                continue\n",
    "\n",
    "            if not all(k in obj for k in (conf_key, pred_key, label_key)):\n",
    "                _handle_invalid(\n",
    "                    f\"Missing keys (need: {conf_key}, {pred_key}, {label_key})\",\n",
    "                    line_no,\n",
    "                    line,\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                conf = float(obj[conf_key])\n",
    "                pred = int(obj[pred_key])\n",
    "                label = int(obj[label_key])\n",
    "            except (TypeError, ValueError):\n",
    "                _handle_invalid(\"Could not parse conf/pred/label into float/int\", line_no, line)\n",
    "                continue\n",
    "\n",
    "            # Confidence should be in [0, 1]\n",
    "            if not (0.0 <= conf <= 1.0):\n",
    "                _handle_invalid(\"Confidence out of [0, 1] range\", line_no, line)\n",
    "                continue\n",
    "\n",
    "            confs.append(conf)\n",
    "            preds.append(pred)\n",
    "            labels.append(label)\n",
    "\n",
    "    if len(confs) == 0:\n",
    "        raise ValueError(\"No valid rows found. Check file path and JSON keys/format.\")\n",
    "\n",
    "    confs_np = np.asarray(confs, dtype=np.float64)\n",
    "    preds_np = np.asarray(preds, dtype=np.int64)\n",
    "    labels_np = np.asarray(labels, dtype=np.int64)\n",
    "    correct_np = (preds_np == labels_np)\n",
    "\n",
    "    # ---- Compute ECE ----\n",
    "    # Uniform bins over [0, 1]. We'll use (lo, hi] except the first bin includes 0.\n",
    "    bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "\n",
    "    bin_counts = np.zeros(n_bins, dtype=np.int64)\n",
    "    bin_acc = np.zeros(n_bins, dtype=np.float64)\n",
    "    bin_conf = np.zeros(n_bins, dtype=np.float64)\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bin_edges[i], bin_edges[i + 1]\n",
    "\n",
    "        if i == 0:\n",
    "            in_bin = (confs_np >= lo) & (confs_np <= hi)\n",
    "        else:\n",
    "            in_bin = (confs_np > lo) & (confs_np <= hi)\n",
    "\n",
    "        count = int(in_bin.sum())\n",
    "        bin_counts[i] = count\n",
    "\n",
    "        if count == 0:\n",
    "            continue\n",
    "\n",
    "        acc = float(correct_np[in_bin].mean())\n",
    "        conf = float(confs_np[in_bin].mean())\n",
    "\n",
    "        bin_acc[i] = acc\n",
    "        bin_conf[i] = conf\n",
    "\n",
    "        weight = count / len(confs_np)\n",
    "        ece += weight * abs(acc - conf)\n",
    "\n",
    "    if not return_details:\n",
    "        return float(ece)\n",
    "\n",
    "    details = {\n",
    "        \"n\": int(len(confs_np)),\n",
    "        \"n_bins\": int(n_bins),\n",
    "        \"bin_edges\": bin_edges.tolist(),\n",
    "        \"bin_counts\": bin_counts.tolist(),\n",
    "        \"bin_accuracy\": bin_acc.tolist(),\n",
    "        \"bin_confidence\": bin_conf.tolist(),\n",
    "    }\n",
    "    return float(ece), details\n",
    "\n",
    "\n",
    "# Example:\n",
    "# ece = ece_from_jsonl(\"preds.jsonl\", n_bins=4)\n",
    "# print(\"ECE:\", ece)\n",
    "#\n",
    "# ece, details = ece_from_jsonl(\"preds.jsonl\", n_bins=4, return_details=True)\n",
    "# print(\"ECE:\", ece)\n",
    "# print(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7a3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def download_preds_artifacts(run, out_dir=\"artifacts\"):\n",
    "    \"\"\"\n",
    "    Download all artifacts from a W&B run whose name starts with 'preds'.\n",
    "\n",
    "    - Does NOT overwrite existing artifacts\n",
    "    - Skips already-downloaded ones\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for artifact in run.logged_artifacts():\n",
    "        name = artifact.name.replace(':', '-')\n",
    "\n",
    "        if not name.startswith(\"preds\"):\n",
    "            continue\n",
    "\n",
    "        target_dir = out_dir / name\n",
    "\n",
    "        # if target_dir.exists():\n",
    "        #     # do not overwrite\n",
    "        #     print(f\"Skipping existing artifact: {name}\")\n",
    "        #     continue\n",
    "\n",
    "        print(f\"Downloading artifact: {name}\")\n",
    "        artifact.download(root=str(target_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7b8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_ece_per_set_once(parent_dir, ece_function, n_bins=4):\n",
    "    \"\"\"\n",
    "    Walk `parent_dir` (containing downloaded W&B artifact folders).\n",
    "    For each artifact folder, find .jsonl files and compute ECE IF:\n",
    "      1) JSONL has a 'conf' field\n",
    "      2) ECE not already computed for that set number\n",
    "\n",
    "    Returns: dict like {set_num: {\"ece\": float, \"jsonl_path\": str, \"artifact_dir\": str}}\n",
    "    \"\"\"\n",
    "    parent_dir = Path(parent_dir)\n",
    "    results = {}\n",
    "    computed_sets = set()\n",
    "\n",
    "    # Example folder name contains \"...-set3-...\" -> capture 3\n",
    "    set_re = re.compile(r\"(?:^|-)set(\\d+)(?:-|$)\")\n",
    "\n",
    "    for artifact_dir in sorted([p for p in parent_dir.iterdir() if p.is_dir()]):\n",
    "        m = set_re.search(artifact_dir.name)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        set_num = int(m.group(1))\n",
    "        if set_num in computed_sets:\n",
    "            # requirement #2: already computed for this set number\n",
    "            continue\n",
    "\n",
    "        # find all .jsonl under this artifact folder\n",
    "        jsonl_paths = list(artifact_dir.rglob(\"*.jsonl\"))\n",
    "        if not jsonl_paths:\n",
    "            continue\n",
    "\n",
    "        # pick the first .jsonl (or iterate until you find one with conf)\n",
    "        picked = None\n",
    "        for jsonl_path in sorted(jsonl_paths):\n",
    "            # requirement #1: 'conf' must exist (check first non-empty JSON line)\n",
    "            has_conf = False\n",
    "            with jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    obj = json.loads(line)\n",
    "                    has_conf = (\"conf\" in obj)\n",
    "                    break\n",
    "\n",
    "            if has_conf:\n",
    "                picked = jsonl_path\n",
    "                break\n",
    "\n",
    "        if picked is None:\n",
    "            # no usable jsonl with conf in this artifact dir\n",
    "            continue\n",
    "\n",
    "        ece = ece_function(str(picked), n_bins=n_bins)\n",
    "        results[set_num] = {\n",
    "            \"ece\": float(ece),\n",
    "            \"jsonl_path\": str(picked),\n",
    "            \"artifact_dir\": str(artifact_dir),\n",
    "        }\n",
    "        computed_sets.add(set_num)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "ENTITY = \"jacoba-california-state-university-east-bay\"\n",
    "PROJECT = \"humaid_ssl\"\n",
    "ARTIFACT_PARENT = \"wandb-preds\"\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "project_obj = api.project(name=PROJECT, entity=ENTITY)\n",
    "sweeps = project_obj.sweeps()\n",
    "\n",
    "best_runs = []\n",
    "\n",
    "for sweep in sweeps:\n",
    "    try:\n",
    "        best = sweep.best_run(order=\"dev_macro-F1\")\n",
    "        if not best:\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get best run for sweep {sweep.id} — skipping. Error: {e}\")\n",
    "        continue\n",
    "\n",
    "    download_preds_artifacts(best, ARTIFACT_PARENT)\n",
    "    ece_dict = compute_ece_per_set_once(ARTIFACT_PARENT, ece_function)\n",
    "\n",
    "    best_runs.append({\n",
    "        \"sweep_id\": sweep.id,\n",
    "        \"sweep_name\": getattr(sweep, \"name\", None),\n",
    "        \"best_run_id\": best.id,\n",
    "        \"best_run_name\": best.name,\n",
    "        \"summary\": best.summary,\n",
    "        \"config\": best.config,\n",
    "        \"ece1\": ece_dict[1][\"ece\"], \n",
    "        \"ece2\": ece_dict[2][\"ece\"], \n",
    "        \"ece3\": ece_dict[3][\"ece\"], \n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(best_runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccbd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sweep name and config to a CSV file\n",
    "# Omit path-related configurations and specific fields from the config dictionary\n",
    "fields_to_exclude = {\"device\", \"set_num\", \"multigpus\", \"artifact_mode\", \"artifact_every\", \"keep_local_ckpt\"}\n",
    "\n",
    "sweep_config_df = pd.DataFrame({\n",
    "    \"sweep_name\": [run[\"sweep_name\"] for run in best_runs],\n",
    "    \"config\": [\n",
    "        {k: v for k, v in run[\"config\"].items() if not k.endswith(\"_path\") and k not in fields_to_exclude}\n",
    "        for run in best_runs\n",
    "    ]\n",
    "})\n",
    "\n",
    "sweep_config_df.to_csv(\"sweep_name_config.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa160fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     {'T': 0.3, 'th': 0.7, 'ssl': True, 'seed': 176...\n",
      "1     {'T': 0.3, 'th': 0.7, 'ssl': True, 'seed': 176...\n",
      "2     {'T': 0.3, 'th': 0.6, 'ssl': True, 'seed': 176...\n",
      "3     {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "4     {'T': 0.3, 'th': 0.7, 'ssl': True, 'seed': 176...\n",
      "5     {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "6     {'T': 0.3, 'th': 0.6, 'ssl': True, 'seed': 176...\n",
      "7     {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "8     {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "9     {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "10    {'T': 1, 'th': 0.8, 'ssl': True, 'seed': 17640...\n",
      "11    {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "12    {'T': 0.3, 'th': 0.6, 'ssl': True, 'seed': 176...\n",
      "13    {'T': 0.3, 'th': 0.6, 'ssl': True, 'seed': 176...\n",
      "14    {'T': 0.3, 'th': 0.7, 'ssl': True, 'seed': 176...\n",
      "15    {'T': 0.3, 'th': 0.6, 'ssl': True, 'seed': 176...\n",
      "16    {'T': 0.3, 'th': 0.6, 'ssl': True, 'seed': 176...\n",
      "17    {'T': 0.5, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "18    {'T': 0.5, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "19    {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "20    {'T': 0.3, 'th': 0.6, 'ssl': True, 'seed': 176...\n",
      "21    {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "22    {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "23    {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "24    {'T': 0.3, 'th': 0.6, 'ssl': True, 'seed': 176...\n",
      "25    {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "26    {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "27    {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "28    {'T': 0.3, 'th': 0.7, 'ssl': True, 'seed': 176...\n",
      "29    {'T': 0.3, 'th': 0.6, 'ssl': True, 'seed': 176...\n",
      "30    {'T': 0.3, 'th': 0.7, 'ssl': True, 'seed': 176...\n",
      "31    {'T': 1, 'th': 0.6, 'ssl': True, 'seed': 17641...\n",
      "32    {'T': 0.3, 'th': 0.6, 'ssl': True, 'seed': 176...\n",
      "33    {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "34    {'T': 0.3, 'th': 0.7, 'ssl': True, 'seed': 176...\n",
      "35    {'T': 0.3, 'th': 0.7, 'ssl': True, 'seed': 176...\n",
      "36    {'T': 0.3, 'th': 0.8, 'ssl': True, 'seed': 176...\n",
      "37    {'T': 0.3, 'th': 0.7, 'ssl': True, 'seed': 176...\n",
      "38    {'T': 0.3, 'th': 0.7, 'ssl': True, 'seed': 176...\n",
      "39    {'T': 0.3, 'th': 0.6, 'ssl': True, 'seed': 176...\n",
      "Name: config, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df[\"config\"].to_csv(\"ece_by_sweep.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba1deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    \"sweep_name\": df[\"sweep_name\"],\n",
    "    \"dev_macro-F1\": df[\"summary\"].apply(lambda s: s.get(\"dev_macro-F1\")),\n",
    "    \"test_macro-F1\": df[\"summary\"].apply(lambda s: s.get(\"test_macro-F1\"))\n",
    "})\n",
    "\n",
    "# Extract the lbcl count from the sweep name (e.g., kerala_floods_2018_50lbcl → 50)\n",
    "df2[\"lbcl_count\"] = df2[\"sweep_name\"].str.extract(r\"_(\\d+)lbcl\").astype(float)\n",
    "\n",
    "# Extract disaster name (everything before _<number>lbcl)\n",
    "df2[\"disaster_name\"] = df2[\"sweep_name\"].str.extract(r\"^(.*)_\\d+lbcl\")\n",
    "\n",
    "# Now sort:\n",
    "df2 = df2.sort_values([\"lbcl_count\", \"disaster_name\"], ascending=[True, True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9488e006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0:\n",
      "0.49733936820431807\n",
      "0.5191025072017287\n",
      "0.4474322680271573\n",
      "0.4966738793228979\n",
      "0.6036988646829177\n",
      "0.5464112371481523\n",
      "0.5419256023827606\n",
      "0.5920581987871771\n",
      "0.6421753635743513\n",
      "0.43018815932671367\n",
      "\n",
      "10.0:\n",
      "0.5719685946389702\n",
      "0.5820763327552231\n",
      "0.50437982285394\n",
      "0.5519814784925708\n",
      "0.6310426842947895\n",
      "0.5895643006829325\n",
      "0.5968836940064904\n",
      "0.6163896344323987\n",
      "0.6694663684212037\n",
      "0.49110881680786383\n",
      "\n",
      "25.0:\n",
      "0.6512848089093568\n",
      "0.5939190763698257\n",
      "0.6395447126493671\n",
      "0.5772570764098851\n",
      "0.6635741437243766\n",
      "0.64602610672146\n",
      "0.6366904832760171\n",
      "0.6613639528348317\n",
      "0.7241081526614853\n",
      "0.564855271916366\n",
      "\n",
      "50.0:\n",
      "0.6526330159256859\n",
      "0.6135023641401601\n",
      "0.6092617177414079\n",
      "0.5936654183611628\n",
      "0.6939616083843202\n",
      "0.6768169689963898\n",
      "0.6681537072950517\n",
      "0.6681890838371644\n",
      "0.7671146722989439\n",
      "0.583763307865972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify unique lbcl values\n",
    "lbcls = sorted(df2[\"lbcl_count\"].unique())\n",
    "\n",
    "tables = {}\n",
    "for lbcl in lbcls:\n",
    "    sub = df2[df2[\"lbcl_count\"] == lbcl].reset_index(drop=True)\n",
    "    tables[lbcl] = sub[\"test_macro-F1\"].tolist()\n",
    "\n",
    "for key, items in tables.items():\n",
    "    print(f\"{key}:\")\n",
    "    for item in items:\n",
    "        print(item)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d3c4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_event_lbcl(name: str):\n",
    "    \"\"\"\n",
    "    Extract event name and lbcl integer from sweep/run name.\n",
    "    Example:\n",
    "      'kerala_floods_2018_50lbcl' ->\n",
    "          ('kerala_floods_2018', 50)\n",
    "    \"\"\"\n",
    "    m = re.search(r\"(.+)_([0-9]+)lbcl$\", name)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    return m.group(1), int(m.group(2))\n",
    "\n",
    "\n",
    "# Build ECE table with parsed fields\n",
    "rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    event, lbcl = parse_event_lbcl(row[\"sweep_name\"])\n",
    "\n",
    "    rows.append({\n",
    "        \"event\": event,\n",
    "        \"lbcl\": lbcl,\n",
    "        \"sweep_name\": row[\"sweep_name\"],\n",
    "        \"ece_set1\": row[\"ece1\"],\n",
    "        \"ece_set2\": row[\"ece2\"],\n",
    "        \"ece_set3\": row[\"ece3\"],\n",
    "    })\n",
    "\n",
    "ece_table = (\n",
    "    pd.DataFrame(rows)\n",
    "    .sort_values(by=[\"lbcl\", \"event\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "ece_table.to_csv(\"ece_by_sweep.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
